{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmx\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLoRALinear\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlx'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    @staticmethod\n",
    "    def from_linear(\n",
    "        linear: nn.Linear,\n",
    "        r: int = 8,\n",
    "        lora_alpha: float = 16,\n",
    "        lora_dropout: float = 0.05,\n",
    "        scale: float = 10.0,\n",
    "    ):\n",
    "        # TODO remove when input_dims and output_dims are attributes\n",
    "        # on linear and quantized linear\n",
    "        output_dims, input_dims = linear.weight.shape\n",
    "        if isinstance(linear, nn.QuantizedLinear):\n",
    "            input_dims *= 32 // linear.bits\n",
    "        lora_lin = LoRALinear(\n",
    "            input_dims=input_dims,\n",
    "            output_dims=output_dims,\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            scale=scale,\n",
    "        )\n",
    "        lora_lin.linear = linear\n",
    "        return lora_lin\n",
    "\n",
    "    def to_linear(self, de_quantize: bool = False):\n",
    "        linear = self.linear\n",
    "        bias = \"bias\" in linear\n",
    "        weight = linear.weight\n",
    "        is_quantized = isinstance(linear, nn.QuantizedLinear)\n",
    "\n",
    "        # Use the same type as the linear weight if not quantized\n",
    "        dtype = weight.dtype\n",
    "\n",
    "        if is_quantized:\n",
    "            dtype = mx.float16\n",
    "            weight = mx.dequantize(\n",
    "                weight,\n",
    "                linear.scales,\n",
    "                linear.biases,\n",
    "                linear.group_size,\n",
    "                linear.bits,\n",
    "            )\n",
    "        output_dims, input_dims = weight.shape\n",
    "        fused_linear = nn.Linear(input_dims, output_dims, bias=bias)\n",
    "\n",
    "        lora_b = (self.scale * self.lora_b.T).astype(dtype)\n",
    "        lora_a = self.lora_a.T.astype(dtype)\n",
    "        fused_linear.weight = weight + lora_b @ lora_a\n",
    "        if bias:\n",
    "            fused_linear.bias = linear.bias\n",
    "\n",
    "        if is_quantized and not de_quantize:\n",
    "            fused_linear = nn.QuantizedLinear.from_linear(\n",
    "                fused_linear,\n",
    "                linear.group_size,\n",
    "                linear.bits,\n",
    "            )\n",
    "\n",
    "        return fused_linear\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims: int,\n",
    "        output_dims: int,\n",
    "        r: int = 8,\n",
    "        lora_alpha: float = 16,\n",
    "        lora_dropout: float = 0.0,\n",
    "        scale: float = 10.0,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Regular linear layer weights\n",
    "        self.linear = nn.Linear(input_dims, output_dims, bias=bias)\n",
    "\n",
    "        self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
    "\n",
    "        # Scale for low-rank update\n",
    "        self.scale = scale * (lora_alpha / r)\n",
    "\n",
    "        # Low rank lora weights\n",
    "        scale = 1 / math.sqrt(input_dims)\n",
    "        self.lora_a = mx.random.uniform(\n",
    "            low=-scale,\n",
    "            high=scale,\n",
    "            shape=(input_dims, r),\n",
    "        )\n",
    "        self.lora_b = mx.zeros(shape=(r, output_dims))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        dtype = self.linear.weight.dtype\n",
    "        if isinstance(self.linear, nn.QuantizedLinear):\n",
    "            dtype = self.linear.scales.dtype\n",
    "        y = self.linear(x.astype(dtype))\n",
    "        z = (self.lora_dropout(x) @ self.lora_a) @ self.lora_b\n",
    "        return y + self.scale * z\n",
    "\n",
    "def linear_to_lora_layers(model: nn.Module, num_lora_layers: int):\n",
    "    \"\"\"\n",
    "    Convert some of the models linear layers to lora layers.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        num_lora_layers (int): The number of blocks to convert to lora layers\n",
    "        starting from the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def check_lora_layers(num_model):\n",
    "        if num_lora_layers > num_model:\n",
    "            raise ValueError(\n",
    "                f\"Requested {num_lora_layers} LoRA layers \"\n",
    "                f\"but the model only has {num_model} layers.\"\n",
    "            )\n",
    "\n",
    "    if model.model_type in [\n",
    "        \"mistral\",\n",
    "        \"llama\",\n",
    "        \"phi\",\n",
    "        \"mixtral\",\n",
    "        \"stablelm_epoch\",\n",
    "        \"qwen2\",\n",
    "    ]:\n",
    "        check_lora_layers(len(model.model.layers))\n",
    "\n",
    "        for l in model.model.layers[len(model.model.layers) - num_lora_layers :]:\n",
    "            l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj)\n",
    "            l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj)\n",
    "            if hasattr(l, \"block_sparse_moe\"):\n",
    "                l.block_sparse_moe.gate = LoRALinear.from_linear(\n",
    "                    l.block_sparse_moe.gate\n",
    "                )\n",
    "    elif model.model_type == \"olmo\":\n",
    "        check_lora_layers(len(model.model.transformer.blocks))\n",
    "\n",
    "        for l in model.model.transformer.blocks[\n",
    "            len(model.model.transformer.blocks) - num_lora_layers :\n",
    "        ]:\n",
    "            l.att_proj = LoRALinear.from_linear(l.att_proj)\n",
    "    elif model.model_type == \"phi-msft\":\n",
    "        check_lora_layers(len(model.transformer.h))\n",
    "\n",
    "        for l in model.transformer.h[len(model.transformer.h) - num_lora_layers :]:\n",
    "            l.mixer.Wqkv = LoRALinear.from_linear(l.mixer.Wqkv)\n",
    "            l.moe.gate = LoRALinear.from_linear(l.moe.gate)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Lora does not support {model.model_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"mlx_model\")\n",
    "\n",
    "# Convert linear layers to lora layers and unfreeze in the process\n",
    "linear_to_lora_layers(model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"adapters.npz\", strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(inp):\n",
    "    return generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        temp=0.8,\n",
    "        max_tokens=500,\n",
    "        prompt=f\"<s>[INST]{inp}[/INST]\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import subprocess\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('/Users/neelredkar/Library/Messages/chat.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_imessage(number, text):\n",
    "    \"\"\"\n",
    "    Sends an iMessage to the specified number with the given text.\n",
    "\n",
    "    Parameters:\n",
    "    - number: The phone number or email address to send the message to. Must be a string.\n",
    "    - text: The message text to send. Must be a string.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # AppleScript command to send an iMessage\n",
    "    applescript_command = f'''tell application \"Messages\"\n",
    "        send \"{text}\" to buddy \"{number}\" of (service 1 whose service type is iMessage)\n",
    "    end tell'''\n",
    "\n",
    "    # Execute the AppleScript command\n",
    "    try:\n",
    "        subprocess.run([\"osascript\", \"-e\", applescript_command], check=True)\n",
    "        print(f\"Message sent to {number}: {text}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to send message to {number}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Path to the chat.db database\n",
    "DATABASE_PATH = \"~/Library/Messages/chat.db\"\n",
    "DATABASE_PATH = os.path.expanduser(DATABASE_PATH)  # Expands the ~ to the full path\n",
    "\n",
    "def get_last_message_id(cursor):\n",
    "    \"\"\"Get the ID of the last message in the database.\"\"\"\n",
    "    cursor.execute(\"SELECT MAX(ROWID) FROM message\")\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "def fetch_new_messages(cursor, last_id):\n",
    "    \"\"\"Fetch new messages since the last known message ID.\"\"\"\n",
    "    cursor.execute(\"SELECT ROWID, handle_id, text FROM message WHERE ROWID > ? ORDER BY ROWID ASC\", (last_id,))\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def get_sender(handle_id, cursor):\n",
    "    \"\"\"Get the sender's phone number or email from the handle_id.\"\"\"\n",
    "    cursor.execute(\"SELECT id FROM handle WHERE ROWID = ?\", (handle_id,))\n",
    "    result = cursor.fetchone()\n",
    "    return result[0] if result else \"Unknown\"\n",
    "\n",
    "def fetch_past_messages(cursor, handle_id, limit=10):\n",
    "    \"\"\"Fetch the past 'limit' messages from a specific sender.\"\"\"\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT text FROM message\n",
    "        WHERE handle_id = ? AND text IS NOT NULL AND cache_roomnames IS NULL\n",
    "        ORDER BY ROWID DESC\n",
    "        LIMIT ?\n",
    "    \"\"\", (handle_id, limit))\n",
    "    return [item[0] for item in cursor.fetchall()][::-1]\n",
    "\n",
    "def main():\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(DATABASE_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Get the last message ID at the start\n",
    "    last_id = get_last_message_id(cursor)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Fetch any new messages\n",
    "            new_messages = fetch_new_messages(cursor, last_id)\n",
    "            if new_messages:\n",
    "                for msg in new_messages:\n",
    "                    rowid, handle_id, text = msg\n",
    "                    if text is not None:\n",
    "                        sender = get_sender(handle_id, cursor)\n",
    "                        past_messages = fetch_past_messages(cursor, handle_id)\n",
    "                        response_text = gen_text(\"\\n\".join(past_messages))  # Assuming gen_text can handle multiple messages\n",
    "                        print(past_messages)\n",
    "                        send_imessage(sender, response_text)\n",
    "                        print(f\"Responded to {sender} based on the last 10 messages.\")\n",
    "                    last_id = rowid  # Update the last seen message ID\n",
    "\n",
    "            # Wait for 5 seconds before checking again\n",
    "            time.sleep(5)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping the script.\")\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yoooo ', ' I need that', 'then come back to me', 'https://www.vice.com/en/article/9bv7qv/the-vice-guide-to-partying-parties', 'This is insane ', 'What is this article ', 'WHY DID Y UNSEND', '😡😡😡😡', 'I did.', 'I read it']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message sent to +16263531677: We’re partying like it’s 1929\n",
      "Responded to +16263531677 based on the last 10 messages.\n",
      "['Yoooo ', ' I need that', 'then come back to me', 'https://www.vice.com/en/article/9bv7qv/the-vice-guide-to-partying-parties', 'This is insane ', 'What is this article ', 'WHY DID Y UNSEND', '😡😡😡😡', 'I did.', 'I read it']\n",
      "Message sent to +16263531677: Fuck\n",
      "Responded to +16263531677 based on the last 10 messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['why you look so sad ', 'please respond to me', 'i need you', 'i can’t live without you', 'i’m ', 'um ', 'yes cat yes ', 'will you give me the cat ', 'will you marry me? ', 'LETS GOOOO']\n",
      "Message sent to +13108803214: I’ve been saving this post for after 6 months\n",
      "Responded to +13108803214 based on the last 10 messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' I need that', 'then come back to me', 'https://www.vice.com/en/article/9bv7qv/the-vice-guide-to-partying-parties', 'This is insane ', 'What is this article ', 'WHY DID Y UNSEND', '😡😡😡😡', 'I did.', 'I read it', 'Oh good']\n",
      "Message sent to +16263531677: I just rewatched the documentary on it\n",
      "Responded to +16263531677 based on the last 10 messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.vice.com/en/article/9bv7qv/the-vice-guide-to-partying-parties', 'This is insane ', 'What is this article ', 'WHY DID Y UNSEND', '😡😡😡😡', 'I did.', 'I read it', 'Oh good', 'Loved “WHY DID Y UNSEND”', 'Loved “I read it”']\n",
      "Message sent to +16263531677: Vice\n",
      "Responded to +16263531677 based on the last 10 messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.vice.com/en/article/9bv7qv/the-vice-guide-to-partying-parties', 'This is insane ', 'What is this article ', 'WHY DID Y UNSEND', '😡😡😡😡', 'I did.', 'I read it', 'Oh good', 'Loved “WHY DID Y UNSEND”', 'Loved “I read it”']\n",
      "Message sent to +16263531677: THAT IS AWESOME\n",
      "Responded to +16263531677 based on the last 10 messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is insane ', 'What is this article ', 'WHY DID Y UNSEND', '😡😡😡😡', 'I did.', 'I read it', 'Oh good', 'Loved “WHY DID Y UNSEND”', 'Loved “I read it”', 'EXSCTLY ']\n",
      "Message sent to +16263531677: I found that article afterwards\n",
      "Responded to +16263531677 based on the last 10 messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WHY DID Y UNSEND', '😡😡😡😡', 'I did.', 'I read it', 'Oh good', 'Loved “WHY DID Y UNSEND”', 'Loved “I read it”', 'EXSCTLY ', 'WE JUST NEED THE TRANS PERSON', 'BAHSGSGSGSG']\n",
      "Message sent to +16263531677: D2\n",
      "Responded to +16263531677 based on the last 10 messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WHY DID Y UNSEND', '😡😡😡😡', 'I did.', 'I read it', 'Oh good', 'Loved “WHY DID Y UNSEND”', 'Loved “I read it”', 'EXSCTLY ', 'WE JUST NEED THE TRANS PERSON', 'BAHSGSGSGSG']\n",
      "Message sent to +16263531677: I’m so happy\n",
      "Responded to +16263531677 based on the last 10 messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping the script.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
